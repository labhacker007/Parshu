# ðŸ§ª GenAI Testing Lab - Complete Implementation

**Date:** January 28, 2026  
**Status:** âœ… **FULLY IMPLEMENTED**

---

## ðŸŽ¯ Overview

A comprehensive testing ground for GenAI models where you can:
- âœ… **Test single models** with custom configurations
- âœ… **Compare multiple models** side-by-side
- âœ… **Apply configurations** from saved configs
- âœ… **Test with guardrails** before production
- âœ… **Evaluate accuracy, reliability, cost, and performance**
- âœ… **Track test history** for analysis

**Purpose:** Test everything before production deployment!

---

## ðŸŽ¨ Features

### 1. Single Model Testing âœ…

**Test one model at a time with full control:**

```
Select Model:
  â””â”€ Choose from all enabled models
  â””â”€ Grouped by provider (OpenAI, Anthropic, Ollama, etc.)
  â””â”€ Shows cost, FREE/LOCAL tags

Use Configuration (Optional):
  â””â”€ Apply saved configuration
  â””â”€ Or use custom parameters

Model Parameters:
  â””â”€ Temperature: 0.0 - 2.0 (Precise â†’ Creative)
  â””â”€ Max Tokens: 1 - 100,000
  â””â”€ Top P: 0.0 - 1.0
  â””â”€ Apply Guardrails: Yes/No

Test Prompt:
  â””â”€ Enter your test prompt
  â””â”€ Can be IOC extraction, summarization, hunt query, etc.

Results Show:
  âœ“ Quality Score (0-100)
  âœ“ Response Time (ms)
  âœ“ Tokens Used
  âœ“ Cost ($)
  âœ“ Full Response
  âœ“ Guardrails Status
```

### 2. Model Comparison âœ…

**Compare 2-5 models side-by-side:**

```
Select Models:
  â””â”€ Check multiple models to compare
  â””â”€ See provider, cost, FREE/LOCAL tags

Same Configuration for All:
  â””â”€ Temperature, Max Tokens, Top P
  â””â”€ Same prompt for fair comparison

Same Prompt:
  â””â”€ Test identical prompt across all models

Results Table Shows:
  âœ“ Model Name & Provider
  âœ“ Quality Score (sortable)
  âœ“ Response Time (sortable)
  âœ“ Tokens Used (sortable)
  âœ“ Cost (sortable)
  âœ“ Success/Failure Status
  âœ“ Expandable to see full response

Winner Highlighted:
  â””â”€ Best quality score
  â””â”€ Fastest response
  â””â”€ Lowest cost
```

### 3. Test History âœ…

**Track all tests for analysis:**

```
History Table Shows:
  âœ“ Timestamp
  âœ“ Model Used
  âœ“ Configuration Used
  âœ“ Quality Score
  âœ“ Response Time
  âœ“ Cost

Stored Locally:
  â””â”€ Last 50 tests saved
  â””â”€ Persists across sessions
  â””â”€ Can clear history

Analysis:
  â””â”€ Compare performance over time
  â””â”€ Identify best models for use cases
  â””â”€ Track cost trends
```

---

## ðŸ“Š Quality Scoring

**How Quality Score is Calculated:**

```javascript
Base Score: 70 points

Bonuses:
  +10 points: Guardrails passed
  +10 points: Tokens used < 80% of max (efficient)
  +10 points: Response time < 3 seconds (fast)

Maximum: 100 points

Example:
  Model A: 90 points (passed guardrails, efficient, fast)
  Model B: 70 points (passed guardrails only)
  Model C: 80 points (passed guardrails, efficient)
  
  Winner: Model A âœ…
```

---

## ðŸ”„ Workflow Examples

### Example 1: Test IOC Extraction

```
Goal: Find best model for IOC extraction

Step 1: Single Model Test
  - Model: GPT-4
  - Config: "ioc_extraction_gpt4" (saved config)
  - Prompt: "Extract IOCs from: [article text]"
  - Run Test
  
  Results:
    Quality: 95/100
    Time: 1,200ms
    Tokens: 1,500
    Cost: $0.045
    
Step 2: Compare with Alternatives
  - Select Models:
    âœ“ GPT-4
    âœ“ GPT-3.5 Turbo
    âœ“ Claude 3 Sonnet
    âœ“ Ollama Llama 3 (FREE)
  - Same Prompt
  - Run Comparison
  
  Results:
    GPT-4:         95/100, 1,200ms, $0.045
    GPT-3.5:       85/100,   800ms, $0.002 âœ… (Best cost)
    Claude:        90/100, 1,500ms, $0.030
    Llama 3:       75/100, 2,000ms, $0.000 âœ… (FREE)
    
Step 3: Decision
  - Production: GPT-3.5 Turbo (good quality, 95% cheaper)
  - Fallback: Ollama Llama 3 (free, acceptable quality)
```

### Example 2: Test Hunt Query Generation

```
Goal: Test different temperatures for hunt queries

Test 1: Temperature = 0.1 (Very Precise)
  Model: GPT-4
  Prompt: "Generate KQL hunt query for ransomware"
  Result: Very specific, focused query âœ…

Test 2: Temperature = 0.5 (Balanced)
  Model: GPT-4
  Prompt: "Generate KQL hunt query for ransomware"
  Result: Good balance, includes variations âœ…

Test 3: Temperature = 0.9 (Creative)
  Model: GPT-4
  Prompt: "Generate KQL hunt query for ransomware"
  Result: Too broad, less focused âŒ

Decision: Use temperature = 0.3 for hunt queries
```

### Example 3: Test Guardrails

```
Goal: Ensure guardrails work before production

Test 1: With Guardrails ON
  Prompt: "Normal IOC extraction request"
  Result: âœ… Passed, extracted IOCs

Test 2: With Guardrails ON
  Prompt: "Malicious prompt injection attempt"
  Result: âŒ Blocked by guardrails

Test 3: With Guardrails OFF (for comparison)
  Prompt: "Malicious prompt injection attempt"
  Result: âš ï¸ Processed (no protection)

Decision: Always use guardrails in production âœ…
```

---

## ðŸ”Œ API Endpoints

### Test Single Model
```
POST /genai/test/single
Body: {
  "model": "openai:gpt-4",
  "prompt": "Extract IOCs from...",
  "temperature": 0.3,
  "max_tokens": 2000,
  "top_p": 0.9,
  "use_guardrails": true,
  "config_id": 123  // Optional
}

Response: {
  "model": "openai:gpt-4",
  "response": "Extracted IOCs: ...",
  "tokens_used": 1500,
  "cost": 0.045,
  "response_time_ms": 1200,
  "guardrails_passed": true,
  "quality_metrics": {
    "response_length": 500,
    "tokens_efficiency": 0.75,
    "cost_efficiency": 0.9,
    "speed_score": 85
  }
}
```

### Compare Models
```
POST /genai/test/compare
Body: {
  "models": [
    "openai:gpt-4",
    "openai:gpt-3.5-turbo",
    "ollama:llama3"
  ],
  "prompt": "Extract IOCs from...",
  "temperature": 0.3,
  "max_tokens": 2000,
  "top_p": 0.9,
  "use_guardrails": true
}

Response: {
  "results": [
    {
      "model": "openai:gpt-4",
      "model_name": "GPT-4",
      "provider": "openai",
      "response": "...",
      "tokens_used": 1500,
      "cost": 0.045,
      "response_time_ms": 1200,
      "quality_metrics": {...}
    },
    // ... more results
  ],
  "total_models": 3,
  "successful": 3,
  "failed": 0
}
```

### Get Test History
```
GET /genai/test/history?limit=50

Response: {
  "history": [
    {
      "id": 123,
      "model": "openai:gpt-4",
      "use_case": "testing",
      "tokens_used": 1500,
      "cost": 0.045,
      "response_time_ms": 1200,
      "was_successful": true,
      "created_at": "2026-01-28T10:00:00Z"
    },
    // ... more history
  ],
  "total": 50
}
```

---

## ðŸ“ Files Created

### Frontend (2 files)
1. **`frontend/src/components/ComprehensiveGenAILab.js`** (NEW - 700 lines)
   - Single model testing tab
   - Model comparison tab
   - Test history tab
   - Quality scoring
   - Local storage for history

2. **`frontend/src/components/ComprehensiveGenAILab.css`** (NEW)
   - Styling for test results
   - Comparison grid layout
   - Metric displays

### Backend (1 file)
3. **`backend/app/genai/testing.py`** (NEW - 400 lines)
   - `/genai/test/single` endpoint
   - `/genai/test/compare` endpoint
   - `/genai/test/history` endpoint
   - Quality metrics calculation
   - Test logging

### Modified Files
4. **`backend/app/main.py`** (MODIFIED)
   - Registered testing router

5. **`frontend/src/pages/Admin.js`** (MODIFIED)
   - Updated to use ComprehensiveGenAILab
   - Changed tab name to "GenAI Testing Lab"

---

## âœ… What You Can Test

### 1. Model Performance
```
âœ“ Response quality
âœ“ Response speed
âœ“ Token efficiency
âœ“ Cost effectiveness
âœ“ Reliability (success rate)
```

### 2. Configuration Impact
```
âœ“ Temperature effects (0.1 vs 0.5 vs 0.9)
âœ“ Max tokens impact
âœ“ Top P variations
âœ“ Saved config vs custom
```

### 3. Guardrails Effectiveness
```
âœ“ Prompt injection protection
âœ“ Content filtering
âœ“ Safety checks
âœ“ Performance impact
```

### 4. Use Case Optimization
```
âœ“ IOC Extraction: Best model? Best temp?
âœ“ Summarization: Which model is fastest?
âœ“ Hunt Queries: Which is most accurate?
âœ“ Chatbot: Which is most conversational?
```

### 5. Cost Analysis
```
âœ“ Compare costs across models
âœ“ Find cheapest option with acceptable quality
âœ“ Calculate ROI for premium models
âœ“ Identify free alternatives
```

---

## ðŸŽ¯ Benefits

### For Admins
- âœ… **Validate before production** - No surprises
- âœ… **Compare costs** - Make informed decisions
- âœ… **Test configurations** - Find optimal settings
- âœ… **Evaluate models** - Choose best for each use case

### For Analysts
- âœ… **Test prompts** - Refine before using
- âœ… **Compare results** - See quality differences
- âœ… **Verify guardrails** - Ensure safety
- âœ… **Learn models** - Understand capabilities

### For the Organization
- âœ… **Cost savings** - Choose right model for each task
- âœ… **Quality assurance** - Test before deploy
- âœ… **Risk mitigation** - Validate guardrails
- âœ… **Performance optimization** - Find fastest/cheapest

---

## ðŸš€ How to Use

### Step 1: Access Testing Lab
```
1. Login to HuntSphere
2. Go to Admin Dashboard
3. Click "GenAI Testing Lab" tab
4. You'll see 3 tabs:
   - Single Model Test
   - Model Comparison
   - Test History
```

### Step 2: Run Single Test
```
1. Select "Single Model Test" tab
2. Choose model (e.g., "Ollama Llama 3")
3. Optional: Select saved configuration
4. Adjust parameters:
   - Temperature: 0.3
   - Max Tokens: 2000
   - Top P: 0.9
   - Guardrails: ON
5. Enter test prompt
6. Click "Run Test"
7. Review results:
   - Quality Score
   - Response Time
   - Tokens Used
   - Cost
   - Full Response
```

### Step 3: Compare Models
```
1. Select "Model Comparison" tab
2. Check 2-5 models to compare
3. Set parameters (same for all)
4. Enter test prompt (same for all)
5. Click "Compare Models"
6. Review comparison table:
   - Sort by quality, time, cost
   - Expand to see responses
   - Identify winner
```

### Step 4: Review History
```
1. Select "Test History" tab
2. See all past tests
3. Analyze trends:
   - Which models used most?
   - Average quality scores
   - Cost trends
4. Clear history if needed
```

---

## ðŸ“Š Status

**Frontend:** âœ… **COMPLETE**  
**Backend:** âœ… **COMPLETE**  
**API:** âœ… **3 ENDPOINTS READY**  
**Integration:** âœ… **COMPLETE**  
**Documentation:** âœ… **COMPREHENSIVE**  

**Overall:** âœ… **100% COMPLETE & READY TO USE**

---

## ðŸŽ‰ Summary

### What Was Built
âœ… **Comprehensive Testing Lab** with 3 tabs  
âœ… **Single Model Testing** with full control  
âœ… **Model Comparison** side-by-side (2-5 models)  
âœ… **Configuration Testing** (saved configs + custom)  
âœ… **Guardrail Testing** (on/off comparison)  
âœ… **Quality Scoring** (0-100 scale)  
âœ… **Test History** (last 50 tests)  
âœ… **Cost Analysis** (compare across models)  
âœ… **Performance Metrics** (time, tokens, efficiency)  

### What You Can Do
âœ… **Test before production** - No surprises  
âœ… **Compare models** - Find best for each use case  
âœ… **Optimize configurations** - Temperature, tokens, etc.  
âœ… **Validate guardrails** - Ensure safety  
âœ… **Analyze costs** - Make informed decisions  
âœ… **Track history** - Learn from past tests  

### Key Features
âœ… **All enabled models** available for testing  
âœ… **All saved configurations** can be applied  
âœ… **All parameters** adjustable  
âœ… **Guardrails** testable  
âœ… **Side-by-side comparison** for 2-5 models  
âœ… **Quality scoring** for objective evaluation  
âœ… **Complete audit trail** in test history  

---

**Your GenAI Testing Lab is now a true testing ground for production validation!** ðŸ§ªðŸŽ‰

**Test everything before deploying to production!**
